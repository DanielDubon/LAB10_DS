{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a638baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from html import unescape\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "sns.set(context=\"notebook\", style=\"ticks\", font_scale=1.0)\n",
    "\n",
    "OUT_DIR = Path(\"./figures_cleaned\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Paleta accesible\n",
    "PALETTE = {\"0\": \"#3182bd\", \"1\": \"#e6550d\"}  # 0 = azul, 1 = naranja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed770495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    _ = stopwords.words('english')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "CLEAN_CFG = {\n",
    "    \"lower\": True,\n",
    "    \"html_unescape\": True,\n",
    "    \"strip_accents\": True,\n",
    "    \"remove_urls\": True,\n",
    "    \"remove_mentions\": True,\n",
    "    \"strip_hashtag_symbol\": False,\n",
    "    \"split_hashtags\": True,\n",
    "    \"remove_emojis\": True,\n",
    "    \"remove_punct\": True,\n",
    "    \"remove_numbers\": False,\n",
    "    \"keep_numbers_set\": {\"911\"},\n",
    "    \"remove_stopwords\": True,\n",
    "    \"remove_domain_stopwords\": True,\n",
    "    \"domain_stopwords\": {\"amp\", \"via\", \"rt\", \"gt\", \"u\", \"im\"},\n",
    "    \"remove_short_tokens\": True,\n",
    "    \"short_min_len\": 2,\n",
    "}\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#([A-Za-z0-9_]+)')\n",
    "PUNCT_RE = re.compile(rf'[{re.escape(string.punctuation)}]')\n",
    "\n",
    "def split_hashtag_token(tok: str) -> str:\n",
    "    tok = tok.replace(\"_\", \" \")\n",
    "    parts = re.findall(r\"[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)|\\d+\", tok)\n",
    "    return \" \".join(parts).lower()\n",
    "\n",
    "def clean_text_pipeline(text: str, cfg=CLEAN_CFG):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\", []\n",
    "    t = text\n",
    "\n",
    "    if cfg[\"html_unescape\"]:\n",
    "        t = unescape(t)\n",
    "    if cfg[\"lower\"]:\n",
    "        t = t.lower()\n",
    "    if cfg[\"strip_accents\"]:\n",
    "        t = unidecode(t)\n",
    "    if cfg[\"remove_urls\"]:\n",
    "        t = URL_RE.sub(\" \", t)\n",
    "    if cfg[\"remove_mentions\"]:\n",
    "        t = MENTION_RE.sub(\" \", t)\n",
    "\n",
    "    if cfg.get(\"split_hashtags\", False):\n",
    "        t = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: split_hashtag_token(m.group(1)), t)\n",
    "    elif cfg[\"strip_hashtag_symbol\"]:\n",
    "        t = HASHTAG_RE.sub(r\"\\1\", t)\n",
    "\n",
    "    if cfg[\"remove_emojis\"]:\n",
    "        try:\n",
    "            t = emoji.replace_emoji(t, replace=\"\")\n",
    "        except Exception:\n",
    "            for e in [e[\"emoji\"] for e in emoji.emoji_list(t)]:\n",
    "                t = t.replace(e, \"\")\n",
    "\n",
    "    if cfg[\"remove_punct\"]:\n",
    "        t = PUNCT_RE.sub(\" \", t)\n",
    "\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    tokens = t.split()\n",
    "\n",
    "    if cfg[\"remove_numbers\"]:\n",
    "        tokens = [tok for tok in tokens if not (tok.isdigit() and tok not in cfg[\"keep_numbers_set\"])]\n",
    "\n",
    "    if cfg[\"remove_stopwords\"]:\n",
    "        tokens = [tok for tok in tokens if tok not in stop_words]\n",
    "\n",
    "    if cfg[\"remove_domain_stopwords\"]:\n",
    "        dom = cfg[\"domain_stopwords\"]\n",
    "        tokens = [tok for tok in tokens if tok not in dom]\n",
    "\n",
    "    if cfg[\"remove_short_tokens\"]:\n",
    "        minlen = cfg[\"short_min_len\"]\n",
    "        tokens = [tok for tok in tokens if (len(tok) >= minlen or tok in {\"a\", \"i\"})]\n",
    "\n",
    "    return \" \".join(tokens), tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e332b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (7613, 5)\n",
      "Nulos por columna (train):\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "Resumen longitudes por clase:\n",
      "       text_char_len                text_word_len             \n",
      "               count    mean median         count  mean median\n",
      "target                                                        \n",
      "0               4342   95.71  101.0          4342  8.28    8.0\n",
      "1               3271  108.11  115.0          3271  9.55    9.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_PATH = Path(\"./train.csv\")\n",
    "TEST_PATH = Path(\"./test.csv\")\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "if TEST_PATH.exists():\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "else:\n",
    "    test = None\n",
    "\n",
    "# Estadísticas iniciales\n",
    "print(\"train shape:\", train.shape)\n",
    "print(\"Nulos por columna (train):\")\n",
    "print(train.isna().sum())\n",
    "\n",
    "# Aplica pipeline\n",
    "train[[\"text_clean_v2\", \"tokens_v2\"]] = train[\"text\"].apply(lambda s: pd.Series(clean_text_pipeline(s)))\n",
    "if test is not None:\n",
    "    test[[\"text_clean_v2\", \"tokens_v2\"]] = test[\"text\"].apply(lambda s: pd.Series(clean_text_pipeline(s)))\n",
    "\n",
    "# Longitudes\n",
    "train[\"text_char_len\"] = train[\"text\"].astype(str).map(len)\n",
    "train[\"text_word_len\"] = train[\"text_clean_v2\"].map(lambda s: len(s.split()))\n",
    "print(\"Resumen longitudes por clase:\")\n",
    "print(train.groupby(\"target\")[[\"text_char_len\", \"text_word_len\"]].agg([\"count\",\"mean\",\"median\"]).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f8b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/2921531401.py:4: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.barplot(x=vc.index.astype(str), y=vc.values, palette=[PALETTE[str(i)] for i in vc.index])\n",
      "/tmp/ipykernel_5290/2921531401.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Distribución de clases\n",
    "plt.figure(figsize=(6,5))\n",
    "vc = train[\"target\"].value_counts().sort_index()\n",
    "ax = sns.barplot(x=vc.index.astype(str), y=vc.values, palette=[PALETTE[str(i)] for i in vc.index])\n",
    "ax.set_title(\"Distribución de clases (0 = no desastre, 1 = desastre)\")\n",
    "ax.set_xlabel(\"target\")\n",
    "ax.set_ylabel(\"Número de tweets\")\n",
    "\n",
    "# Anotar porcentaje encima de cada barra\n",
    "total = vc.sum()\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()-200\n",
    "    pct = height / total * 100\n",
    "    ax.annotate(f\"{int(height):,}\\n({pct:.1f}%)\", (p.get_x() + p.get_width() / 2, height),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"01_class_distribution_pct.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2afe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/3179439299.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Grafica 5 - Longitud de tweets: hist + KDE por clase\n",
    "plt.figure(figsize=(10,5))\n",
    "max_len = int(min(280, train[\"text_char_len\"].max()))\n",
    "bins = np.linspace(0, max_len, 35)\n",
    "\n",
    "# Histograma apilado (semi-transparente) y KDEs\n",
    "for cls in sorted(train[\"target\"].unique()):\n",
    "    subset = train.loc[train[\"target\"] == cls, \"text_char_len\"]\n",
    "    plt.hist(subset.clip(upper=max_len), bins=bins, alpha=0.35, density=False,\n",
    "             label=f\"class {cls}\", color=PALETTE[str(cls)])\n",
    "    sns.kdeplot(subset.clip(upper=max_len), bw_adjust=1.0, fill=False, color=PALETTE[str(cls)], linewidth=2)\n",
    "\n",
    "plt.title(\"Distribución de longitud de tweets (caracteres) por clase\")\n",
    "plt.xlabel(\"Longitud (caracteres, recortada a 280)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend(title=\"Clase\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"02_tweet_length_by_class.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f124edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/796574359.py:23: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/tmp/ipykernel_5290/796574359.py:23: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Grafica 6 - Top unigrams por clase (usando tokens_v2)\n",
    "def top_tokens_by_class(df, cls, topn=20):\n",
    "    c = Counter()\n",
    "    for toks in df.loc[df[\"target\"]==cls, \"tokens_v2\"]:\n",
    "        if isinstance(toks, list):\n",
    "            c.update(toks)\n",
    "    return c.most_common(topn)\n",
    "\n",
    "top0 = top_tokens_by_class(train, 0, topn=25)\n",
    "top1 = top_tokens_by_class(train, 1, topn=25)\n",
    "\n",
    "def plot_top_tokens(pairs, title, outname, color):\n",
    "    tokens = [p for p,_ in pairs]\n",
    "    freqs = [f for _,f in pairs]\n",
    "    dfp = pd.DataFrame({\"token\": tokens, \"freq\": freqs})\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.barplot(data=dfp, x=\"freq\", y=\"token\", color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frecuencia\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / outname, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_top_tokens(top1, \"Top 25 tokens — Desastre (1)\", \"03_top_unigrams_desastre.png\", PALETTE[\"1\"])\n",
    "plot_top_tokens(top0, \"Top 25 tokens — No desastre (0)\", \"04_top_unigrams_nodesastre.png\", PALETTE[\"0\"])\n",
    "\n",
    "# Guardar CSVs para inspección\n",
    "pd.DataFrame(top1, columns=[\"token\",\"freq\"]).to_csv(OUT_DIR / \"top_unigrams_desastre.csv\", index=False)\n",
    "pd.DataFrame(top0, columns=[\"token\",\"freq\"]).to_csv(OUT_DIR / \"top_unigrams_nodesastre.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d91b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/796574359.py:23: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/tmp/ipykernel_5290/796574359.py:23: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def bigrams_from_tokens(tokens):\n",
    "    return [\" \".join(pair) for pair in zip(tokens, tokens[1:])]\n",
    "\n",
    "def top_bigrams_by_class(df, cls, topn=20):\n",
    "    c = Counter()\n",
    "    for toks in df.loc[df[\"target\"]==cls, \"tokens_v2\"]:\n",
    "        if isinstance(toks, list) and len(toks) >= 2:\n",
    "            c.update(bigrams_from_tokens(toks))\n",
    "    return c.most_common(topn)\n",
    "\n",
    "top0_bi = top_bigrams_by_class(train, 0, topn=25)\n",
    "top1_bi = top_bigrams_by_class(train, 1, topn=25)\n",
    "\n",
    "plot_top_tokens(top1_bi, \"Top 25 bigrams — Desastre (1)\", \"05_top_bigrams_desastre.png\", PALETTE[\"1\"])\n",
    "plot_top_tokens(top0_bi, \"Top 25 bigrams — No desastre (0)\", \"06_top_bigrams_nodesastre.png\", PALETTE[\"0\"])\n",
    "\n",
    "pd.DataFrame(top1_bi, columns=[\"bigram\",\"freq\"]).to_csv(OUT_DIR / \"top_bigrams_desastre.csv\", index=False)\n",
    "pd.DataFrame(top0_bi, columns=[\"bigram\",\"freq\"]).to_csv(OUT_DIR / \"top_bigrams_nod.esastre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698b0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/4219566516.py:11: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "FONT_PATH = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "\n",
    "def generar_wc(text, outpath, font_path=FONT_PATH):\n",
    "    wc = WordCloud(width=1000, height=600, background_color=\"white\", font_path=font_path).generate(text)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "texto1 = \" \".join(train.loc[train[\"target\"]==1, \"text_clean_v2\"].astype(str).values)\n",
    "texto0 = \" \".join(train.loc[train[\"target\"]==0, \"text_clean_v2\"].astype(str).values)\n",
    "generar_wc(texto1, OUT_DIR / \"07_wordcloud_desastre.png\")\n",
    "generar_wc(texto0, OUT_DIR / \"08_wordcloud_nod.esastre.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd2fadda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Análisis TF-IDF de Palabras Distintivas ===\n",
      "\n",
      "--- Top 25 palabras más distintivas de Desastre (1) ---\n",
      "       token  diff_class1  tfidf_class1\n",
      "  california     0.012119      0.012555\n",
      "        fire     0.011983      0.019149\n",
      "     suicide     0.010303      0.011045\n",
      "   hiroshima     0.010156      0.010257\n",
      "      killed     0.009079      0.009398\n",
      "    disaster     0.008729      0.012543\n",
      "        news     0.008666      0.012918\n",
      "      police     0.008488      0.011385\n",
      "    wildfire     0.008476      0.008663\n",
      "       fires     0.008475      0.010116\n",
      "     bombing     0.008076      0.008354\n",
      "       storm     0.007933      0.011196\n",
      "       train     0.007379      0.008677\n",
      "       mh370     0.007150      0.007150\n",
      "    northern     0.006644      0.006644\n",
      "    families     0.006500      0.007155\n",
      "     typhoon     0.006434      0.006527\n",
      "   buildings     0.006368      0.009538\n",
      "       homes     0.006344      0.006674\n",
      "       crash     0.006063      0.009211\n",
      "       japan     0.005839      0.006158\n",
      "      atomic     0.005764      0.005950\n",
      "    accident     0.005759      0.008096\n",
      "      attack     0.005684      0.008301\n",
      "legionnaires     0.005641      0.005641\n",
      "\n",
      "--- Top 25 palabras más distintivas de No Desastre (0) ---\n",
      "      token  diff_class0  tfidf_class0\n",
      "       like     0.009773      0.018367\n",
      "       body     0.007865      0.009519\n",
      "       love     0.007597      0.009088\n",
      "        new     0.007189      0.012628\n",
      "        get     0.006852      0.013275\n",
      "       full     0.005875      0.006936\n",
      "        let     0.005439      0.006845\n",
      "        lol     0.005237      0.006346\n",
      "       know     0.004933      0.007889\n",
      "      would     0.004912      0.008715\n",
      "        got     0.004665      0.007767\n",
      "        see     0.004593      0.007557\n",
      "       want     0.004552      0.005932\n",
      "  screaming     0.004498      0.005679\n",
      "    wrecked     0.004357      0.004904\n",
      "       ruin     0.004353      0.004480\n",
      "        one     0.004218      0.010633\n",
      "       bags     0.004181      0.004320\n",
      "     bloody     0.004164      0.005239\n",
      "        day     0.004124      0.007941\n",
      "       blew     0.004013      0.004270\n",
      "      crush     0.004001      0.005012\n",
      "traumatised     0.003951      0.004454\n",
      "       time     0.003889      0.007160\n",
      "      think     0.003867      0.006120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/3599074692.py:72: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Análisis de palabras distintivas usando TF-IDF\n",
    "# Este análisis muestra qué palabras son más características de cada clase\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"=== Análisis TF-IDF de Palabras Distintivas ===\")\n",
    "\n",
    "# Separar textos por clase\n",
    "texts_class0 = train.loc[train[\"target\"] == 0, \"text_clean_v2\"].astype(str).tolist()\n",
    "texts_class1 = train.loc[train[\"target\"] == 1, \"text_clean_v2\"].astype(str).tolist()\n",
    "\n",
    "# Calcular TF-IDF por clase (documento = clase)\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=2, ngram_range=(1, 1))\n",
    "tfidf_matrix = vectorizer.fit_transform(texts_class0 + texts_class1)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Promediar TF-IDF scores para cada clase\n",
    "tfidf_class0 = tfidf_matrix[:len(texts_class0)].mean(axis=0).A1\n",
    "tfidf_class1 = tfidf_matrix[len(texts_class0):].mean(axis=0).A1\n",
    "\n",
    "# Crear DataFrame con scores\n",
    "df_tfidf = pd.DataFrame({\n",
    "    'token': feature_names,\n",
    "    'tfidf_class0': tfidf_class0,\n",
    "    'tfidf_class1': tfidf_class1\n",
    "})\n",
    "\n",
    "# Calcular diferencia (qué tan distintiva es cada palabra)\n",
    "df_tfidf['diff_class1'] = df_tfidf['tfidf_class1'] - df_tfidf['tfidf_class0']\n",
    "df_tfidf['diff_class0'] = df_tfidf['tfidf_class0'] - df_tfidf['tfidf_class1']\n",
    "\n",
    "# Top palabras más distintivas de cada clase\n",
    "top_distinct_class1 = df_tfidf.nlargest(25, 'diff_class1')[['token', 'diff_class1', 'tfidf_class1']]\n",
    "top_distinct_class0 = df_tfidf.nlargest(25, 'diff_class0')[['token', 'diff_class0', 'tfidf_class0']]\n",
    "\n",
    "print(\"\\n--- Top 25 palabras más distintivas de Desastre (1) ---\")\n",
    "print(top_distinct_class1.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Top 25 palabras más distintivas de No Desastre (0) ---\")\n",
    "print(top_distinct_class0.to_string(index=False))\n",
    "\n",
    "# Guardar resultados\n",
    "top_distinct_class1.to_csv(OUT_DIR / \"top_distinctive_words_class1.csv\", index=False)\n",
    "top_distinct_class0.to_csv(OUT_DIR / \"top_distinctive_words_class0.csv\", index=False)\n",
    "\n",
    "# Visualización: palabras más distintivas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Clase 1 - Desastre\n",
    "ax1 = axes[0]\n",
    "y_pos = np.arange(len(top_distinct_class1))\n",
    "ax1.barh(y_pos, top_distinct_class1['diff_class1'].values, color=PALETTE[\"1\"])\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(top_distinct_class1['token'].values)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Score TF-IDF Distintivo (mayor = más característico de clase 1)')\n",
    "ax1.set_title('Top 25 palabras más distintivas — Desastre (1)')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Clase 0 - No Desastre\n",
    "ax2 = axes[1]\n",
    "y_pos = np.arange(len(top_distinct_class0))\n",
    "ax2.barh(y_pos, top_distinct_class0['diff_class0'].values, color=PALETTE[\"0\"])\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(top_distinct_class0['token'].values)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Score TF-IDF Distintivo (mayor = más característico de clase 0)')\n",
    "ax2.set_title('Top 25 palabras más distintivas — No Desastre (0)')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"10_distinctive_words_tfidf.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
